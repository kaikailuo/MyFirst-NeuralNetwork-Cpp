# MyFirst-NeuralNetwork-Cpp

 大一学生自主编写的三层神经网络（C++），支持训练/测试/激活函数切换。

##  项目简介

这是我在大一下期间独立完成的一个简单神经网络实现项目。它用 C++ 编写，模拟了一个简单的三层神经网络（输入层-隐藏层-输出层），支持前向传播、反向传播、训练、损失计算、激活函数选择等功能。

虽然功能基础，但整个神经网络结构、权重初始化、传播机制、训练逻辑，全部由我手动实现，是对 AI 理论的一次深入动手实践。

---

##  项目结构

```
MyFirst-NeuralNetwork-Cpp/
├── README.md         # 项目说明文档
├── NeutralNetwork.h  # 神经网络类头文件
├── NeutralNetwork.cpp# 神经网络类实现文件
├── 测试.cpp           # main 函数，含训练与测试逻辑
├── train1.txt~4.txt  # 每种激活函数组合的训练日志
├── case1.txt~4.txt   # 每种设置的输出结果
```

---

##  功能特性

- 前向传播（支持不同激活函数）
- 反向传播更新权重
- 交叉熵损失函数
- 支持 Sigmoid / Tanh / ReLU 激活函数选择
- 可选的动态学习率衰减
- 输出每轮训练详细信息至文件，便于调试和分析

---

##  输出说明

程序结束后，会生成如下输出文件：
- `case*.txt`：每种设置的训练前/后测试输出
- `train*.txt`：训练过程的日志（每 100 轮一次）

---

##  示例说明

### 输入：
```
test_input={0.6, 0.8, 0.9},test_target={1}
```
### 输出（示例，实际因权重不同而变化）：二分类时 大于0.5 判断标签为1
```
Before training: 0.815966 
After training: 0.907376 

```

---

##  激活函数对比实验

你可以看到 `main` 函数中共测试了 4 组配置：
1. 固定权重 + Sigmoid
2. 固定权重 + Tanh
3. 固定权重 + ReLU
4. 随机权重 + Sigmoid

方便观察激活函数在同一数据集上的表现差异。

---

##  开发者备注

这是我进入 AI 领域的第一步。虽然实现简单，但我深刻体会到神经网络从数据输入到损失计算、再到权重更新的整个链路。

我没有用任何深度学习框架，只依靠 C++ 标准库自己搭建，未来希望能尝试手写卷积网络或迁移到 Python+PyTorch。

---
